#!/usr/bin/env python3
"""
ACORNå‚æ•°è‡ªåŠ¨æœç´¢ - å‰©ä½™5ä¸ªæ•°æ®é›†
åŸºäºarxivæµ‹è¯•ç»“æœä¼˜åŒ–çš„å‚æ•°èŒƒå›´

ç”¨æ³•:
  python auto_param_search_remaining.py                    # è¿è¡Œæ‰€æœ‰5ä¸ªæ•°æ®é›†
  python auto_param_search_remaining.py yfcc               # åªè¿è¡ŒæŒ‡å®šæ•°æ®é›†
  python auto_param_search_remaining.py yfcc LAION1M       # è¿è¡Œå¤šä¸ªæ•°æ®é›†
"""

import os
import sys
import time
import json
import csv
import subprocess
from pathlib import Path
from datetime import datetime

# ==================== æ•°æ®é›†é…ç½® ====================

# é€šç”¨å‚æ•°èŒƒå›´ï¼ˆåŸºäºarxivæµ‹è¯•ç»“æœä¼˜åŒ–ï¼‰
# - ç§»é™¤M_beta=128ï¼ˆM=32æ—¶ä¼šå¤±è´¥ï¼‰
# - ä¿ç•™gamma=24ï¼ˆæŸäº›åœºæ™¯å¯èƒ½éœ€è¦ï¼‰
DEFAULT_PARAMS = {
    "Ms": [32, 48, 64],
    "M_betas": [48, 64, 96],  # ç§»é™¤128é¿å…å¤±è´¥
    "gammas": [4, 8, 12, 24],
}

# å¤§æ•°æ®é›†ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°èŒƒå›´ï¼ˆå‡å°‘æ„å»ºæ—¶é—´ï¼‰
LARGE_DATASET_PARAMS = {
    "Ms": [32, 48, 64],
    "M_betas": [48, 64],      # æ›´å°‘çš„M_beta
    "gammas": [4, 8, 12],     # æ›´å°‘çš„gamma
}

DATASETS_CONFIG = {
    "yfcc": {
        "N": 1000000,
        "data_dir": "/home/remote/u7905817/benchmarks/datasets/discrete/yfcc",
        "scenarios": ["equal", "or", "and"],
        **DEFAULT_PARAMS,
    },
    "LAION1M": {
        "N": 1000448,
        "data_dir": "/home/remote/u7905817/benchmarks/datasets/discrete/LAION1M",
        "scenarios": ["and"],  # LAION1Måªæ”¯æŒandåœºæ™¯
        **DEFAULT_PARAMS,
    },
    "tripclick": {
        "N": 1055976,
        "data_dir": "/home/remote/u7905817/benchmarks/datasets/discrete/tripclick",
        "scenarios": ["equal", "and"],  # tripclickä¸æ”¯æŒoråœºæ™¯
        **DEFAULT_PARAMS,
    },
    "ytb_video": {
        "N": 5000000,
        "data_dir": "/home/remote/u7905817/benchmarks/datasets/discrete/ytb_video",
        "scenarios": ["equal", "or", "and"],
        **LARGE_DATASET_PARAMS,  # å¤§æ•°æ®é›†ä½¿ç”¨æ›´å°å‚æ•°èŒƒå›´
    },
}

# å…¨å±€å¸¸é‡
K = 10  # Recall@10

# ==================== è¾…åŠ©å‡½æ•° ====================

def log(msg):
    """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—"""
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}", flush=True)

def check_dataset_files(dataset, config):
    """æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    data_dir = config['data_dir']
    required_files = [
        f"{data_dir}/{dataset}_base.fvecs",
        f"{data_dir}/label_base.txt",
    ]

    for scenario in config['scenarios']:
        required_files.extend([
            f"{data_dir}/{dataset}_query_{scenario}.fvecs",
            f"{data_dir}/{dataset}_query_{scenario}.txt",
            f"{data_dir}/{dataset}_gt_{scenario}.txt",
        ])

    missing = [f for f in required_files if not os.path.exists(f)]

    if missing:
        log(f"âŒ æ•°æ®é›† {dataset} ç¼ºå°‘æ–‡ä»¶:")
        for f in missing:
            log(f"   - {f}")
        return False

    log(f"âœ… æ•°æ®é›† {dataset} æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    return True

def load_progress(progress_file):
    """åŠ è½½è¿›åº¦"""
    if os.path.exists(progress_file):
        with open(progress_file, 'r') as f:
            data = json.load(f)
            return set(tuple(x) for x in data.get('completed', []))
    return set()

def save_progress(progress_file, completed_tasks):
    """ä¿å­˜è¿›åº¦"""
    output_dir = os.path.dirname(progress_file)
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    with open(progress_file, 'w') as f:
        json.dump({
            'completed': list(completed_tasks),
            'last_update': datetime.now().isoformat(),
            'total': len(completed_tasks)
        }, f, indent=2)

def load_build_info(summary_file):
    """ä»å·²æœ‰çš„summary.csvåŠ è½½æ„å»ºä¿¡æ¯"""
    build_info = {}

    if os.path.exists(summary_file):
        try:
            with open(summary_file, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    key = (int(float(row['M'])), int(float(row['M_beta'])), int(float(row['gamma'])))
                    if key not in build_info and row.get('build_time_s'):
                        try:
                            build_info[key] = {
                                'build_time': float(row['build_time_s']),
                                'index_size': float(row['index_size_mb'])
                            }
                        except:
                            pass
        except:
            pass

    return build_info

def init_summary_file(summary_file):
    """åˆå§‹åŒ–æ±‡æ€»æ–‡ä»¶"""
    if not os.path.exists(summary_file):
        output_dir = os.path.dirname(summary_file)
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        with open(summary_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'M', 'M_beta', 'gamma', 'scenario',
                'build_time_s', 'qps', 'qps_no_filter',
                'recall@10', 'index_size_mb', 'status', 'timestamp'
            ])

def get_index_size(index_path):
    """è·å–ç´¢å¼•æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
    try:
        size_bytes = os.path.getsize(index_path)
        return size_bytes / (1024 * 1024)
    except:
        return 0.0

def parse_search_csv(csv_file):
    """ä»CSVæ–‡ä»¶æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆæœ€ä¼˜Recallå€¼ï¼‰"""
    try:
        import csv as csv_module
        with open(csv_file, 'r') as f:
            reader = csv_module.DictReader(f)
            best_recall = 0.0
            best_qps = 0.0
            best_qps_no_filter = 0.0

            for row in reader:
                recall = float(row['Recall'])
                if recall > best_recall:
                    best_recall = recall
                    best_qps = float(row['QPS'])
                    best_qps_no_filter = float(row['QPS_no_filter'])

            if best_recall > 0:
                return {
                    'recall@10': best_recall,
                    'qps': best_qps,
                    'qps_no_filter': best_qps_no_filter
                }
    except Exception as e:
        pass
    return None

# ==================== ä¸»æµç¨‹ ====================

def build_index(dataset, M, M_beta, gamma, config, output_base):
    """æ„å»ºç´¢å¼•"""
    # C++ç¨‹åºä¼šåœ¨output_pathä¸‹å†åŠ ä¸€å±‚{dataset}/å­ç›®å½•
    # æ‰€ä»¥è¿™é‡Œä¼ å…¥indicesç›®å½•ï¼ˆä¸åŒ…å«datasetï¼‰ï¼Œç”±C++ç¨‹åºæ‹¼æ¥
    indices_base = f"{output_base}/indices"
    os.makedirs(indices_base, exist_ok=True)

    # åˆ›å»ºdatasetå­ç›®å½•ï¼Œé¿å…C++å†™æ–‡ä»¶æ—¶æŠ¥é”™
    dataset_dir = f"{indices_base}/{dataset}"
    os.makedirs(dataset_dir, exist_ok=True)

    log_file = f"{dataset_dir}/M={M}_Mb={M_beta}_gamma={gamma}_build.log"

    data_dir = config['data_dir']
    N = config['N']

    cmd = [
        '../build/demos/build_acorn_index',
        str(N), str(gamma), f"{data_dir}/{dataset}_base.fvecs",
        str(M), str(M_beta), indices_base, dataset
    ]

    env = os.environ.copy()
    env['OMP_NUM_THREADS'] = '16'

    start_time = time.time()

    try:
        with open(log_file, 'w') as f:
            subprocess.run(cmd, env=env, check=True,
                         stdout=f, stderr=subprocess.STDOUT,
                         timeout=3600)

        build_time = time.time() - start_time
        index_path = f"{dataset_dir}/hybrid_M={M}_Mb={M_beta}_gamma={gamma}.json"
        index_size = get_index_size(index_path)

        return True, build_time, index_size

    except subprocess.TimeoutExpired:
        log(f"   â±ï¸  æ„å»ºè¶…æ—¶ï¼ˆ1å°æ—¶ï¼‰")
        return False, 0, 0
    except subprocess.CalledProcessError as e:
        log(f"   âŒ æ„å»ºå¤±è´¥: {e}")
        return False, 0, 0
    except Exception as e:
        log(f"   âŒ å¼‚å¸¸: {e}")
        return False, 0, 0

def search_index(dataset, M, M_beta, gamma, scenario, config, output_base):
    """æµ‹è¯•ç´¢å¼•æœç´¢æ€§èƒ½"""
    # C++ç¨‹åºæœŸæœ›index_pathæ˜¯ä¸åŒ…å«datasetçš„åŸºç¡€ç›®å½•
    indices_base = f"{output_base}/indices"
    results_dir = f"{output_base}/results/{dataset}/{scenario}"
    os.makedirs(results_dir, exist_ok=True)

    log_file = f"{results_dir}/M={M}_Mb={M_beta}_gamma={gamma}_search.log"

    data_dir = config['data_dir']
    N = config['N']

    cmd = [
        '../build/demos/search_acorn_index',
        str(N), str(gamma), dataset,
        str(M), str(M_beta), indices_base, scenario, results_dir,
        f"{data_dir}/{dataset}_base.fvecs",
        f"{data_dir}/label_base.txt",
        f"{data_dir}/{dataset}_query_{scenario}.fvecs",
        f"{data_dir}/{dataset}_query_{scenario}.txt",
        f"{data_dir}/{dataset}_gt_{scenario}.txt",
        str(K)
    ]

    env = os.environ.copy()
    env['debugSearchFlag'] = '0'

    try:
        with open(log_file, 'w') as f:
            subprocess.run(cmd, env=env, check=True,
                         stdout=f, stderr=subprocess.STDOUT,
                         timeout=1800)

        csv_file = f"{results_dir}/M={M}_M_beta={M_beta}_gamma={gamma}_result.csv"
        metrics = parse_search_csv(csv_file)
        return True, metrics

    except subprocess.TimeoutExpired:
        log(f"   â±ï¸  æœç´¢è¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰")
        return False, None
    except subprocess.CalledProcessError as e:
        log(f"   âŒ æœç´¢å¤±è´¥: {e}")
        return False, None
    except Exception as e:
        log(f"   âŒ å¼‚å¸¸: {e}")
        return False, None

def run_param_search(dataset):
    """è¿è¡Œå•ä¸ªæ•°æ®é›†çš„å‚æ•°æœç´¢"""

    if dataset not in DATASETS_CONFIG:
        log(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset}")
        log(f"æ”¯æŒçš„æ•°æ®é›†: {', '.join(DATASETS_CONFIG.keys())}")
        return

    config = DATASETS_CONFIG[dataset]

    log("="*70)
    log(f"ACORNå‚æ•°æœç´¢ - {dataset.upper()} æ•°æ®é›†")
    log(f"æ•°æ®è§„æ¨¡: N={config['N']:,}")
    log(f"æµ‹è¯•åœºæ™¯: {', '.join(config['scenarios'])}")
    log(f"å‚æ•°èŒƒå›´: M={config['Ms']}, M_beta={config['M_betas']}, gamma={config['gammas']}")
    log("="*70)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_dataset_files(dataset, config):
        log(f"âš ï¸  è·³è¿‡æ•°æ®é›† {dataset}")
        return

    # è¾“å‡ºè·¯å¾„
    output_base = f"/home/remote/u7905817/benchmarks/discrete/ACORN/data/param_search_{dataset}"
    summary_file = f"{output_base}/summary.csv"
    progress_file = f"{output_base}/progress.json"

    # åˆå§‹åŒ–
    init_summary_file(summary_file)
    completed_tasks = load_progress(progress_file)
    build_info = load_build_info(summary_file)

    # è®¡ç®—æœ‰æ•ˆå‚æ•°ç»„åˆ
    valid_combinations = []
    for M in config['Ms']:
        for M_beta in config['M_betas']:
            if M_beta < M:
                continue
            for gamma in config['gammas']:
                if M_beta > 2 * M * gamma:
                    continue
                valid_combinations.append((M, M_beta, gamma))

    total_tasks = len(valid_combinations) * len(config['scenarios'])

    log(f"æœ‰æ•ˆå‚æ•°ç»„åˆ: {len(valid_combinations)}")
    log(f"æ€»ä»»åŠ¡æ•°: {total_tasks} (åŒ…å«{len(config['scenarios'])}ä¸ªåœºæ™¯)")
    log(f"å·²å®Œæˆ: {len([t for t in completed_tasks if len(t) == 4 and t[-1] in config['scenarios']])}")

    start_time = time.time()
    current_task = 0

    # éå†æ‰€æœ‰å‚æ•°ç»„åˆ
    for M, M_beta, gamma in valid_combinations:

        log(f"\n{'='*70}")
        log(f"å‚æ•°ç»„åˆ: M={M}, M_beta={M_beta}, gamma={gamma}")
        log(f"{'='*70}")

        # æ­¥éª¤1: æ„å»ºç´¢å¼•
        build_key = (M, M_beta, gamma, 'build')
        index_path = f"{output_base}/indices/{dataset}/hybrid_M={M}_Mb={M_beta}_gamma={gamma}.json"
        index_exists = os.path.exists(index_path) and os.path.getsize(index_path) > 1000

        if build_key not in completed_tasks or not index_exists:
            if not index_exists and build_key in completed_tasks:
                log(f"âš ï¸  ç´¢å¼•æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´ï¼Œé‡æ–°æ„å»º...")
                completed_tasks.discard(build_key)
            else:
                log(f"ğŸ”¨ æ„å»ºç´¢å¼•...")

            success, build_time, index_size = build_index(dataset, M, M_beta, gamma, config, output_base)

            if success:
                log(f"   âœ… æ„å»ºæˆåŠŸ: {build_time:.1f}ç§’, å¤§å°: {index_size:.1f}MB")
                completed_tasks.add(build_key)
                save_progress(progress_file, completed_tasks)
            else:
                log(f"   âŒ æ„å»ºå¤±è´¥ï¼Œè·³è¿‡æ­¤å‚æ•°ç»„åˆ")
                continue
        else:
            log(f"â­ï¸  ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
            param_key = (M, M_beta, gamma)
            if param_key in build_info:
                build_time = build_info[param_key]['build_time']
                index_size = build_info[param_key]['index_size']
                log(f"   ğŸ“‹ æ¢å¤æ„å»ºä¿¡æ¯: {build_time:.1f}ç§’, {index_size:.1f}MB")
            else:
                build_time = 0
                index_size = get_index_size(index_path)
                log(f"   ğŸ“ ç´¢å¼•å¤§å°: {index_size:.1f}MB (æ„å»ºæ—¶é—´æœªè®°å½•)")

        # æ­¥éª¤2: æµ‹è¯•æ‰€æœ‰åœºæ™¯
        for scenario in config['scenarios']:
            current_task += 1
            task_key = (M, M_beta, gamma, scenario)

            if task_key in completed_tasks:
                log(f"â­ï¸  [{current_task}/{total_tasks}] åœºæ™¯ {scenario.upper()} å·²å®Œæˆ")
                continue

            progress_pct = (current_task / total_tasks) * 100
            elapsed = time.time() - start_time
            eta = (elapsed / current_task) * (total_tasks - current_task) if current_task > 0 else 0

            log(f"\nğŸ” [{current_task}/{total_tasks}] ({progress_pct:.1f}%) æµ‹è¯•åœºæ™¯: {scenario.upper()}")
            log(f"â±ï¸  å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")

            success, metrics = search_index(dataset, M, M_beta, gamma, scenario, config, output_base)

            if success and metrics:
                log(f"   âœ… æµ‹è¯•æˆåŠŸ:")
                log(f"      QPS (with filter): {metrics['qps']:.2f}")
                log(f"      Recall@10: {metrics['recall@10']:.4f}")

                with open(summary_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        M, M_beta, gamma, scenario,
                        f"{build_time:.2f}",
                        f"{metrics['qps']:.2f}",
                        f"{metrics['qps_no_filter']:.2f}",
                        f"{metrics['recall@10']:.4f}",
                        f"{index_size:.1f}",
                        'success',
                        datetime.now().isoformat()
                    ])

                completed_tasks.add(task_key)
                save_progress(progress_file, completed_tasks)
            else:
                log(f"   âŒ æµ‹è¯•å¤±è´¥")
                with open(summary_file, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        M, M_beta, gamma, scenario,
                        0, 0, 0, 0, 0,
                        'failed',
                        datetime.now().isoformat()
                    ])

    # å®Œæˆæ€»ç»“
    total_time = time.time() - start_time
    log(f"\n{'='*70}")
    log(f"ğŸ‰ {dataset.upper()} æ•°æ®é›†å‚æ•°æœç´¢å®Œæˆï¼")
    log(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)")
    log(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {summary_file}")
    log(f"{'='*70}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        # è¿è¡ŒæŒ‡å®šçš„æ•°æ®é›†
        datasets_to_run = sys.argv[1:]
        for dataset in datasets_to_run:
            run_param_search(dataset)
            log(f"\n{'='*70}\n")
    else:
        # è¿è¡Œæ‰€æœ‰å‰©ä½™æ•°æ®é›†
        log("ğŸš€ å¼€å§‹æ‰€æœ‰å‰©ä½™æ•°æ®é›†çš„å‚æ•°æœç´¢")
        log(f"æ•°æ®é›†åˆ—è¡¨: {', '.join(DATASETS_CONFIG.keys())}")
        log("")

        for dataset in DATASETS_CONFIG.keys():
            run_param_search(dataset)
            log(f"\n{'='*70}\n")

        log("âœ… æ‰€æœ‰æ•°æ®é›†å‚æ•°æœç´¢å®Œæˆï¼")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
ACORNè‡ªåŠ¨å‚æ•°æœç´¢ - Arxivæ•°æ®é›†
å®Œæ•´æµç¨‹ï¼šæ„å»ºç´¢å¼• â†’ æµ‹è¯•æ€§èƒ½ â†’ æ‰¾åˆ°æœ€ä¼˜å‚æ•°
"""

import subprocess
import os
import csv
import time
import json
from datetime import datetime
from pathlib import Path

# ==================== é…ç½® ====================

# æ•°æ®é›†é…ç½®
DATASET = "arxiv"
N = 132687  # arxivæ•°æ®é‡

# æ•°æ®è·¯å¾„ï¼ˆäº‘ç«¯å®é™…è·¯å¾„ï¼‰
DATA_DIR = "/home/remote/u7905817/benchmarks/datasets/discrete/arxiv"
BASE_FILE = f"{DATA_DIR}/arxiv_base.fvecs"
BASE_LABEL_FILE = f"{DATA_DIR}/label_base.txt"

# Queryåœºæ™¯
SCENARIOS = ["equal", "or", "and"]  # ä¸‰ç§æŸ¥è¯¢ç±»å‹

# è¾“å‡ºè·¯å¾„ï¼ˆåœ¨ACORNç›®å½•ä¸‹ï¼‰
OUTPUT_BASE = "/home/remote/u7905817/benchmarks/discrete/ACORN/data/param_search_arxiv"
RESULTS_DIR = f"{OUTPUT_BASE}/results"
INDEX_DIR = f"{OUTPUT_BASE}/indices"

# å‚æ•°èŒƒå›´ - é’ˆå¯¹arxivï¼ˆ13ä¸‡æ•°æ®ï¼‰ä¼˜åŒ–
Ms = list(range(12, 49, 4))           # [12, 16, 20, 24, 28, 32, 36, 40, 44, 48]
M_betas = list(range(12, 65, 4))      # [12, 16, 20, ..., 60, 64]
gammas = [1, 2, 4]                    # æµ‹è¯•3ä¸ªgammaå€¼

# æœç´¢å‚æ•°
K = 10  # Top-K

# è¿›åº¦æ–‡ä»¶
PROGRESS_FILE = f"{OUTPUT_BASE}/progress.json"
SUMMARY_FILE = f"{OUTPUT_BASE}/summary.csv"

# ==================== å·¥å…·å‡½æ•° ====================

def log(msg):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}")

def load_progress():
    """åŠ è½½è¿›åº¦"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            data = json.load(f)
            return set(tuple(x) for x in data.get('completed', []))
    return set()

def save_progress(completed_tasks):
    """ä¿å­˜è¿›åº¦"""
    Path(OUTPUT_BASE).mkdir(parents=True, exist_ok=True)
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({
            'completed': list(completed_tasks),
            'last_update': datetime.now().isoformat(),
            'total': len(completed_tasks)
        }, f, indent=2)

def init_summary_file():
    """åˆå§‹åŒ–æ±‡æ€»æ–‡ä»¶"""
    if not os.path.exists(SUMMARY_FILE):
        Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)
        with open(SUMMARY_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'M', 'M_beta', 'gamma', 'scenario',
                'build_time_s', 'search_time_ms',
                'recall@1', 'recall@10', 'recall@100',
                'index_size_mb', 'status', 'timestamp'
            ])

def parse_build_log(log_file):
    """è§£ææ„å»ºæ—¥å¿—ï¼Œæå–æ„å»ºæ—¶é—´"""
    try:
        with open(log_file, 'r') as f:
            content = f.read()
            for line in content.split('\n'):
                if 'Create gamma index in time:' in line:
                    return float(line.split(':')[-1].strip())
    except:
        pass
    return 0.0

def parse_search_log(log_file):
    """è§£ææœç´¢æ—¥å¿—ï¼Œæå–æ€§èƒ½æŒ‡æ ‡"""
    try:
        with open(log_file, 'r') as f:
            content = f.read()

            metrics = {
                'search_time_ms': 0.0,
                'recall@1': 0.0,
                'recall@10': 0.0,
                'recall@100': 0.0
            }

            for line in content.split('\n'):
                if 'Average search time' in line or 'Query time' in line:
                    # æå–æ—¶é—´ï¼ˆå¯èƒ½æ˜¯msæˆ–sï¼‰
                    parts = line.split(':')[-1].strip().split()
                    if parts:
                        metrics['search_time_ms'] = float(parts[0])

                elif 'Recall@1' in line or 'R@1' in line:
                    parts = line.split(':')[-1].strip().split()
                    if parts:
                        metrics['recall@1'] = float(parts[0])

                elif 'Recall@10' in line or 'R@10' in line:
                    parts = line.split(':')[-1].strip().split()
                    if parts:
                        metrics['recall@10'] = float(parts[0])

                elif 'Recall@100' in line or 'R@100' in line:
                    parts = line.split(':')[-1].strip().split()
                    if parts:
                        metrics['recall@100'] = float(parts[0])

            return metrics
    except:
        pass
    return None

def get_index_size(index_path):
    """è·å–ç´¢å¼•æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
    try:
        size_bytes = os.path.getsize(index_path)
        return size_bytes / (1024 * 1024)
    except:
        return 0.0

# ==================== ä¸»æµç¨‹ ====================

def check_data_files():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    log("æ£€æŸ¥æ•°æ®æ–‡ä»¶...")

    required_files = [BASE_FILE, BASE_LABEL_FILE]

    for scenario in SCENARIOS:
        required_files.extend([
            f"{DATA_DIR}/arxiv_query_{scenario}.fvecs",
            f"{DATA_DIR}/arxiv_query_{scenario}.txt",
            f"{DATA_DIR}/arxiv_gt_{scenario}.txt"
        ])

    missing = [f for f in required_files if not os.path.exists(f)]

    if missing:
        log(f"âŒ ç¼ºå°‘ä»¥ä¸‹æ•°æ®æ–‡ä»¶ï¼š")
        for f in missing:
            log(f"   - {f}")
        log(f"\nè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨æ­£ç¡®ä½ç½®ï¼")
        return False

    log("âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶å°±ç»ª")
    return True

def build_index(M, M_beta, gamma):
    """æ„å»ºç´¢å¼•"""
    dir_name = f"M={M}_Mb={M_beta}_gamma={gamma}"
    index_dir = os.path.join(INDEX_DIR, DATASET, dir_name)
    os.makedirs(index_dir, exist_ok=True)

    log_file = os.path.join(index_dir, 'build.log')

    cmd = [
        '../build/demos/build_acorn_index',
        str(N), str(gamma), BASE_FILE,
        str(M), str(M_beta), INDEX_DIR, DATASET
    ]

    env = os.environ.copy()
    env['debugSearchFlag'] = '0'

    start_time = time.time()

    try:
        with open(log_file, 'w') as f:
            subprocess.run(cmd, env=env, check=True,
                         stdout=f, stderr=subprocess.STDOUT,
                         timeout=3600)

        build_time = time.time() - start_time

        # è·å–ç´¢å¼•æ–‡ä»¶è·¯å¾„
        index_path = f"{INDEX_DIR}/{DATASET}/hybrid_M={M}_Mb={M_beta}_gamma={gamma}.json"
        index_size = get_index_size(index_path)

        return True, build_time, index_size

    except subprocess.TimeoutExpired:
        log(f"   â±ï¸  æ„å»ºè¶…æ—¶ï¼ˆ1å°æ—¶ï¼‰")
        return False, 0, 0
    except subprocess.CalledProcessError as e:
        log(f"   âŒ æ„å»ºå¤±è´¥: {e}")
        return False, 0, 0
    except Exception as e:
        log(f"   âŒ å¼‚å¸¸: {e}")
        return False, 0, 0

def search_index(M, M_beta, gamma, scenario):
    """æµ‹è¯•ç´¢å¼•æœç´¢æ€§èƒ½"""
    dir_name = f"M={M}_Mb={M_beta}_gamma={gamma}"
    result_dir = os.path.join(RESULTS_DIR, DATASET, scenario, dir_name)
    os.makedirs(result_dir, exist_ok=True)

    log_file = os.path.join(result_dir, 'search.log')

    query_file = f"{DATA_DIR}/arxiv_query_{scenario}.fvecs"
    query_label_file = f"{DATA_DIR}/arxiv_query_{scenario}.txt"
    gt_path = f"{DATA_DIR}/arxiv_gt_{scenario}.txt"

    cmd = [
        '../build/demos/search_acorn_index',
        str(N), str(gamma), DATASET,
        str(M), str(M_beta), INDEX_DIR, scenario, RESULTS_DIR,
        BASE_FILE, BASE_LABEL_FILE,
        query_file, query_label_file, gt_path, str(K)
    ]

    env = os.environ.copy()
    env['debugSearchFlag'] = '0'

    try:
        with open(log_file, 'w') as f:
            subprocess.run(cmd, env=env, check=True,
                         stdout=f, stderr=subprocess.STDOUT,
                         timeout=1800)

        # è§£ææ€§èƒ½æŒ‡æ ‡
        metrics = parse_search_log(log_file)
        return True, metrics

    except subprocess.TimeoutExpired:
        log(f"   â±ï¸  æœç´¢è¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰")
        return False, None
    except subprocess.CalledProcessError as e:
        log(f"   âŒ æœç´¢å¤±è´¥: {e}")
        return False, None
    except Exception as e:
        log(f"   âŒ å¼‚å¸¸: {e}")
        return False, None

def main():
    """ä¸»å‡½æ•°"""
    log("="*70)
    log("ACORNè‡ªåŠ¨å‚æ•°æœç´¢ - Arxivæ•°æ®é›†")
    log("="*70)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_data_files():
        return

    # åˆå§‹åŒ–
    init_summary_file()
    completed_tasks = load_progress()

    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    total_tasks = 0
    valid_combinations = []

    for M in Ms:
        for M_beta in M_betas:
            if M_beta < M:
                continue
            for gamma in gammas:
                if M_beta > 2 * M * gamma:
                    continue
                valid_combinations.append((M, M_beta, gamma))
                total_tasks += len(SCENARIOS)

    log(f"\nå‚æ•°é…ç½®ï¼š")
    log(f"  MèŒƒå›´: {Ms}")
    log(f"  M_betaèŒƒå›´: {M_betas}")
    log(f"  gammaèŒƒå›´: {gammas}")
    log(f"  æŸ¥è¯¢åœºæ™¯: {SCENARIOS}")
    log(f"\næœ‰æ•ˆå‚æ•°ç»„åˆ: {len(valid_combinations)}")
    log(f"æ€»ä»»åŠ¡æ•°: {total_tasks} (åŒ…å«{len(SCENARIOS)}ä¸ªåœºæ™¯)")
    log(f"å·²å®Œæˆ: {len(completed_tasks)}")
    log(f"å‰©ä½™: {total_tasks - len(completed_tasks)}\n")

    start_time = time.time()
    current_task = 0

    # éå†æ‰€æœ‰å‚æ•°ç»„åˆ
    for M, M_beta, gamma in valid_combinations:

        log(f"\n{'='*70}")
        log(f"å‚æ•°ç»„åˆ: M={M}, M_beta={M_beta}, gamma={gamma}")
        log(f"{'='*70}")

        # æ­¥éª¤1: æ„å»ºç´¢å¼•ï¼ˆå¦‚æœè¿˜æ²¡æ„å»ºï¼‰
        build_key = (M, M_beta, gamma, 'build')

        if build_key not in completed_tasks:
            log(f"ğŸ”¨ æ„å»ºç´¢å¼•...")
            success, build_time, index_size = build_index(M, M_beta, gamma)

            if success:
                log(f"   âœ… æ„å»ºæˆåŠŸ: {build_time:.1f}ç§’, å¤§å°: {index_size:.1f}MB")
                completed_tasks.add(build_key)
                save_progress(completed_tasks)
            else:
                log(f"   âŒ æ„å»ºå¤±è´¥ï¼Œè·³è¿‡æ­¤å‚æ•°ç»„åˆ")
                continue
        else:
            log(f"â­ï¸  ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
            build_time = 0
            index_size = 0

        # æ­¥éª¤2: æµ‹è¯•æ‰€æœ‰åœºæ™¯
        for scenario in SCENARIOS:
            current_task += 1
            task_key = (M, M_beta, gamma, scenario)

            if task_key in completed_tasks:
                log(f"â­ï¸  [{current_task}/{total_tasks}] åœºæ™¯ {scenario.upper()} å·²å®Œæˆ")
                continue

            progress_pct = (current_task / total_tasks) * 100
            elapsed = time.time() - start_time
            eta = (elapsed / current_task) * (total_tasks - current_task) if current_task > 0 else 0

            log(f"\nğŸ” [{current_task}/{total_tasks}] ({progress_pct:.1f}%) æµ‹è¯•åœºæ™¯: {scenario.upper()}")
            log(f"â±ï¸  å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")

            success, metrics = search_index(M, M_beta, gamma, scenario)

            if success and metrics:
                log(f"   âœ… æµ‹è¯•æˆåŠŸ:")
                log(f"      æŸ¥è¯¢æ—¶é—´: {metrics['search_time_ms']:.2f}ms")
                log(f"      Recall@10: {metrics['recall@10']:.4f}")

                # è®°å½•ç»“æœ
                with open(SUMMARY_FILE, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        M, M_beta, gamma, scenario,
                        f"{build_time:.2f}",
                        f"{metrics['search_time_ms']:.2f}",
                        f"{metrics['recall@1']:.4f}",
                        f"{metrics['recall@10']:.4f}",
                        f"{metrics['recall@100']:.4f}",
                        f"{index_size:.1f}",
                        'success',
                        datetime.now().isoformat()
                    ])

                completed_tasks.add(task_key)
                save_progress(completed_tasks)
            else:
                log(f"   âŒ æµ‹è¯•å¤±è´¥")
                # ä¹Ÿè®°å½•å¤±è´¥çš„ç»“æœ
                with open(SUMMARY_FILE, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        M, M_beta, gamma, scenario,
                        0, 0, 0, 0, 0, 0,
                        'failed',
                        datetime.now().isoformat()
                    ])

    # å®Œæˆæ€»ç»“
    total_time = time.time() - start_time
    log(f"\n{'='*70}")
    log(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    log(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)")
    log(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {SUMMARY_FILE}")
    log(f"{'='*70}")

    # åˆ†ææœ€ä¼˜å‚æ•°
    analyze_results()

def analyze_results():
    """åˆ†æç»“æœï¼Œæ‰¾å‡ºæœ€ä¼˜å‚æ•°"""
    log(f"\n{'='*70}")
    log("ğŸ“ˆ æœ€ä¼˜å‚æ•°åˆ†æ")
    log(f"{'='*70}\n")

    try:
        import pandas as pd
        df = pd.read_csv(SUMMARY_FILE)
        df = df[df['status'] == 'success']

        for scenario in SCENARIOS:
            log(f"\nåœºæ™¯: {scenario.upper()}")
            log("-" * 50)

            scenario_df = df[df['scenario'] == scenario]

            if len(scenario_df) == 0:
                log("  æ²¡æœ‰æˆåŠŸçš„ç»“æœ")
                continue

            # æŒ‰recall@10æ’åºï¼Œå–å‰5
            top5 = scenario_df.nlargest(5, 'recall@10')

            log(f"  {'M':>3} {'Mb':>3} {'Î³':>2}  {'R@10':>7}  {'æ—¶é—´(ms)':>9}  {'å†…å­˜(MB)':>9}")
            log("  " + "-" * 50)

            for _, row in top5.iterrows():
                log(f"  {int(row['M']):>3} {int(row['M_beta']):>3} {int(row['gamma']):>2}  "
                    f"{row['recall@10']:>7.4f}  {row['search_time_ms']:>9.2f}  "
                    f"{row['index_size_mb']:>9.1f}")

            # æ¨èæœ€ä¼˜ï¼ˆrecallæœ€é«˜ä¸”æ—¶é—´åˆç†ï¼‰
            best = top5.iloc[0]
            log(f"\n  ğŸ† æ¨è: M={int(best['M'])}, M_beta={int(best['M_beta'])}, gamma={int(best['gamma'])}")
            log(f"     Recall@10={best['recall@10']:.4f}, æ—¶é—´={best['search_time_ms']:.2f}ms")

    except ImportError:
        log("æç¤ºï¼šå®‰è£…pandaså¯ä»¥è‡ªåŠ¨åˆ†ææœ€ä¼˜å‚æ•°")
        log("  pip install pandas")
    except Exception as e:
        log(f"åˆ†æå¤±è´¥: {e}")

if __name__ == '__main__':
    main()

#!/usr/bin/env python3
"""
ACORNè‡ªåŠ¨å‚æ•°æœç´¢ - Arxivæ•°æ®é›†
å®Œæ•´æµç¨‹ï¼šæ„å»ºç´¢å¼• â†’ æµ‹è¯•æ€§èƒ½ â†’ æ‰¾åˆ°æœ€ä¼˜å‚æ•°
"""

import subprocess
import os
import csv
import time
import json
from datetime import datetime
from pathlib import Path

# ==================== é…ç½® ====================

# æ•°æ®é›†é…ç½®
DATASET = "arxiv"
N = 132687  # arxivæ•°æ®é‡

# æ•°æ®è·¯å¾„ï¼ˆäº‘ç«¯å®é™…è·¯å¾„ï¼‰
DATA_DIR = "/home/remote/u7905817/benchmarks/datasets/discrete/arxiv"
BASE_FILE = f"{DATA_DIR}/arxiv_base.fvecs"
BASE_LABEL_FILE = f"{DATA_DIR}/label_base.txt"

# Queryåœºæ™¯
SCENARIOS = ["equal", "or", "and"]  # ä¸‰ç§æŸ¥è¯¢ç±»å‹

# è¾“å‡ºè·¯å¾„ï¼ˆåœ¨ACORNç›®å½•ä¸‹ï¼‰
OUTPUT_BASE = "/home/remote/u7905817/benchmarks/discrete/ACORN/data/param_search_arxiv"
RESULTS_DIR = f"{OUTPUT_BASE}/results"
INDEX_DIR = f"{OUTPUT_BASE}/indices"

# å‚æ•°èŒƒå›´ - åŸºäºæµ‹è¯•ç»“æœä¼˜åŒ–ï¼ˆgammaéœ€è¦è¶³å¤Ÿå¤§æ‰èƒ½è¾¾åˆ°Recall>0.80ï¼‰
Ms = [32, 48, 64]                     # [32, 48, 64] - å»æ‰è¿‡å°çš„å€¼
M_betas = [48, 64, 96, 128]           # [48, 64, 96, 128] - å»æ‰è¿‡å°çš„å€¼
gammas = [4, 8, 12, 24]               # [4, 8, 12, 24] - gamma>=4æ‰èƒ½è¾¾åˆ°ç›®æ ‡Recall

# æœç´¢å‚æ•°
K = 10  # Top-K

# è¿›åº¦æ–‡ä»¶
PROGRESS_FILE = f"{OUTPUT_BASE}/progress.json"
SUMMARY_FILE = f"{OUTPUT_BASE}/summary.csv"

# ==================== å·¥å…·å‡½æ•° ====================

def log(msg):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}")

def load_progress():
    """åŠ è½½è¿›åº¦"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            data = json.load(f)
            return set(tuple(x) for x in data.get('completed', []))
    return set()

def save_progress(completed_tasks):
    """ä¿å­˜è¿›åº¦"""
    Path(OUTPUT_BASE).mkdir(parents=True, exist_ok=True)
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({
            'completed': list(completed_tasks),
            'last_update': datetime.now().isoformat(),
            'total': len(completed_tasks)
        }, f, indent=2)

def load_build_info():
    """ä»å·²æœ‰çš„summary.csvåŠ è½½æ„å»ºä¿¡æ¯ï¼ˆç”¨äºæ¢å¤build_timeå’Œindex_sizeï¼‰"""
    build_info = {}  # {(M, M_beta, gamma): {'build_time': x, 'index_size': y}}

    if os.path.exists(SUMMARY_FILE):
        try:
            with open(SUMMARY_FILE, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    key = (int(float(row['M'])), int(float(row['M_beta'])), int(float(row['gamma'])))
                    if key not in build_info and row.get('build_time_s'):
                        try:
                            build_info[key] = {
                                'build_time': float(row['build_time_s']),
                                'index_size': float(row['index_size_mb'])
                            }
                        except:
                            pass
        except:
            pass

    return build_info

def init_summary_file():
    """åˆå§‹åŒ–æ±‡æ€»æ–‡ä»¶"""
    if not os.path.exists(SUMMARY_FILE):
        Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)
        with open(SUMMARY_FILE, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                'M', 'M_beta', 'gamma', 'scenario',
                'build_time_s', 'qps', 'qps_no_filter',
                'recall@10', 'index_size_mb', 'status', 'timestamp'
            ])

def parse_build_log(log_file):
    """è§£ææ„å»ºæ—¥å¿—ï¼Œæå–æ„å»ºæ—¶é—´"""
    try:
        with open(log_file, 'r') as f:
            content = f.read()
            for line in content.split('\n'):
                if 'Create gamma index in time:' in line:
                    return float(line.split(':')[-1].strip())
    except:
        pass
    return 0.0

def parse_search_csv(csv_file):
    """ä»CSVæ–‡ä»¶æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆæœ€ä¼˜Recallå€¼ï¼‰"""
    try:
        import csv
        with open(csv_file, 'r') as f:
            reader = csv.DictReader(f)
            best_recall = 0.0
            best_qps = 0.0
            best_qps_no_filter = 0.0

            for row in reader:
                recall = float(row['Recall'])
                if recall > best_recall:
                    best_recall = recall
                    best_qps = float(row['QPS'])
                    best_qps_no_filter = float(row['QPS_no_filter'])

            if best_recall > 0:
                return {
                    'recall@10': best_recall,
                    'qps': best_qps,
                    'qps_no_filter': best_qps_no_filter
                }
    except Exception as e:
        pass
    return None

def get_index_size(index_path):
    """è·å–ç´¢å¼•æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
    try:
        size_bytes = os.path.getsize(index_path)
        return size_bytes / (1024 * 1024)
    except:
        return 0.0

# ==================== ä¸»æµç¨‹ ====================

def check_data_files():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    log("æ£€æŸ¥æ•°æ®æ–‡ä»¶...")

    required_files = [BASE_FILE, BASE_LABEL_FILE]

    for scenario in SCENARIOS:
        required_files.extend([
            f"{DATA_DIR}/arxiv_query_{scenario}.fvecs",
            f"{DATA_DIR}/arxiv_query_{scenario}.txt",
            f"{DATA_DIR}/arxiv_gt_{scenario}.txt"
        ])

    missing = [f for f in required_files if not os.path.exists(f)]

    if missing:
        log(f"âŒ ç¼ºå°‘ä»¥ä¸‹æ•°æ®æ–‡ä»¶ï¼š")
        for f in missing:
            log(f"   - {f}")
        log(f"\nè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨æ­£ç¡®ä½ç½®ï¼")
        return False

    log("âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶å°±ç»ª")
    return True

def build_index(M, M_beta, gamma):
    """æ„å»ºç´¢å¼•"""
    # Ensure base index directory exists for ACORN to write index files
    os.makedirs(os.path.join(INDEX_DIR, DATASET), exist_ok=True)

    dir_name = f"M={M}_Mb={M_beta}_gamma={gamma}"
    index_dir = os.path.join(INDEX_DIR, DATASET, dir_name)
    os.makedirs(index_dir, exist_ok=True)

    log_file = os.path.join(index_dir, 'build.log')

    cmd = [
        '../build/demos/build_acorn_index',
        str(N), str(gamma), BASE_FILE,
        str(M), str(M_beta), INDEX_DIR, DATASET
    ]

    env = os.environ.copy()
    env['debugSearchFlag'] = '0'

    start_time = time.time()

    try:
        with open(log_file, 'w') as f:
            subprocess.run(cmd, env=env, check=True,
                         stdout=f, stderr=subprocess.STDOUT,
                         timeout=3600)

        build_time = time.time() - start_time

        # è·å–ç´¢å¼•æ–‡ä»¶è·¯å¾„
        index_path = f"{INDEX_DIR}/{DATASET}/hybrid_M={M}_Mb={M_beta}_gamma={gamma}.json"
        index_size = get_index_size(index_path)

        return True, build_time, index_size

    except subprocess.TimeoutExpired:
        log(f"   â±ï¸  æ„å»ºè¶…æ—¶ï¼ˆ1å°æ—¶ï¼‰")
        return False, 0, 0
    except subprocess.CalledProcessError as e:
        log(f"   âŒ æ„å»ºå¤±è´¥: {e}")
        return False, 0, 0
    except Exception as e:
        log(f"   âŒ å¼‚å¸¸: {e}")
        return False, 0, 0

def search_index(M, M_beta, gamma, scenario):
    """æµ‹è¯•ç´¢å¼•æœç´¢æ€§èƒ½"""
    # è¾“å‡ºè·¯å¾„åº”è¯¥åŒ…å«datasetå’Œscenarioï¼ˆä¸C++ç¨‹åºæœŸæœ›ä¸€è‡´ï¼‰
    output_path = os.path.join(RESULTS_DIR, DATASET, scenario)
    os.makedirs(output_path, exist_ok=True)

    log_file = os.path.join(output_path, f'M={M}_Mb={M_beta}_gamma={gamma}_search.log')

    query_file = f"{DATA_DIR}/arxiv_query_{scenario}.fvecs"
    query_label_file = f"{DATA_DIR}/arxiv_query_{scenario}.txt"
    gt_path = f"{DATA_DIR}/arxiv_gt_{scenario}.txt"

    cmd = [
        '../build/demos/search_acorn_index',
        str(N), str(gamma), DATASET,
        str(M), str(M_beta), INDEX_DIR, scenario, output_path,  # ä¼ é€’å®Œæ•´è·¯å¾„
        BASE_FILE, BASE_LABEL_FILE,
        query_file, query_label_file, gt_path, str(K)
    ]

    env = os.environ.copy()
    env['debugSearchFlag'] = '0'

    try:
        with open(log_file, 'w') as f:
            subprocess.run(cmd, env=env, check=True,
                         stdout=f, stderr=subprocess.STDOUT,
                         timeout=1800)

        # ä»CSVæ–‡ä»¶è§£ææ€§èƒ½æŒ‡æ ‡
        csv_file = os.path.join(output_path, f"M={M}_M_beta={M_beta}_gamma={gamma}_result.csv")
        metrics = parse_search_csv(csv_file)
        return True, metrics

    except subprocess.TimeoutExpired:
        log(f"   â±ï¸  æœç´¢è¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰")
        return False, None
    except subprocess.CalledProcessError as e:
        log(f"   âŒ æœç´¢å¤±è´¥: {e}")
        return False, None
    except Exception as e:
        log(f"   âŒ å¼‚å¸¸: {e}")
        return False, None

def main():
    """ä¸»å‡½æ•°"""
    log("="*70)
    log("ACORNè‡ªåŠ¨å‚æ•°æœç´¢ - Arxivæ•°æ®é›†")
    log("="*70)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not check_data_files():
        return

    # åˆå§‹åŒ–
    init_summary_file()
    completed_tasks = load_progress()
    build_info = load_build_info()  # åŠ è½½å·²æœ‰çš„æ„å»ºä¿¡æ¯

    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    total_tasks = 0
    valid_combinations = []

    for M in Ms:
        for M_beta in M_betas:
            if M_beta < M:
                continue
            for gamma in gammas:
                if M_beta > 2 * M * gamma:
                    continue
                valid_combinations.append((M, M_beta, gamma))
                total_tasks += len(SCENARIOS)

    log(f"\nå‚æ•°é…ç½®ï¼š")
    log(f"  MèŒƒå›´: {Ms}")
    log(f"  M_betaèŒƒå›´: {M_betas}")
    log(f"  gammaèŒƒå›´: {gammas}")
    log(f"  æŸ¥è¯¢åœºæ™¯: {SCENARIOS}")
    log(f"\næœ‰æ•ˆå‚æ•°ç»„åˆ: {len(valid_combinations)}")
    log(f"æ€»ä»»åŠ¡æ•°: {total_tasks} (åŒ…å«{len(SCENARIOS)}ä¸ªåœºæ™¯)")
    log(f"å·²å®Œæˆ: {len(completed_tasks)}")
    log(f"å‰©ä½™: {total_tasks - len(completed_tasks)}\n")

    start_time = time.time()
    current_task = 0

    # éå†æ‰€æœ‰å‚æ•°ç»„åˆ
    for M, M_beta, gamma in valid_combinations:

        log(f"\n{'='*70}")
        log(f"å‚æ•°ç»„åˆ: M={M}, M_beta={M_beta}, gamma={gamma}")
        log(f"{'='*70}")

        # æ­¥éª¤1: æ„å»ºç´¢å¼•ï¼ˆå¦‚æœè¿˜æ²¡æ„å»ºï¼‰
        build_key = (M, M_beta, gamma, 'build')

        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
        index_path = f"{INDEX_DIR}/{DATASET}/hybrid_M={M}_Mb={M_beta}_gamma={gamma}.json"
        index_exists = os.path.exists(index_path) and os.path.getsize(index_path) > 1000  # è‡³å°‘1KB

        if build_key not in completed_tasks or not index_exists:
            if not index_exists and build_key in completed_tasks:
                log(f"âš ï¸  ç´¢å¼•æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´ï¼Œé‡æ–°æ„å»º...")
                completed_tasks.discard(build_key)  # ä»å·²å®Œæˆåˆ—è¡¨ç§»é™¤
            else:
                log(f"ğŸ”¨ æ„å»ºç´¢å¼•...")

            success, build_time, index_size = build_index(M, M_beta, gamma)

            if success:
                log(f"   âœ… æ„å»ºæˆåŠŸ: {build_time:.1f}ç§’, å¤§å°: {index_size:.1f}MB")
                completed_tasks.add(build_key)
                save_progress(completed_tasks)
            else:
                log(f"   âŒ æ„å»ºå¤±è´¥ï¼Œè·³è¿‡æ­¤å‚æ•°ç»„åˆ")
                continue
        else:
            log(f"â­ï¸  ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
            # å°è¯•ä»ä¹‹å‰çš„summaryæ¢å¤æ„å»ºä¿¡æ¯
            param_key = (M, M_beta, gamma)
            if param_key in build_info:
                build_time = build_info[param_key]['build_time']
                index_size = build_info[param_key]['index_size']
                log(f"   ğŸ“‹ æ¢å¤æ„å»ºä¿¡æ¯: {build_time:.1f}ç§’, {index_size:.1f}MB")
            else:
                # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œåªèƒ½è¯»å–æ–‡ä»¶å¤§å°
                build_time = 0  # æ— æ³•æ¢å¤
                index_size = get_index_size(index_path)
                log(f"   ğŸ“ ç´¢å¼•å¤§å°: {index_size:.1f}MB (æ„å»ºæ—¶é—´æœªè®°å½•)")

        # æ­¥éª¤2: æµ‹è¯•æ‰€æœ‰åœºæ™¯
        for scenario in SCENARIOS:
            current_task += 1
            task_key = (M, M_beta, gamma, scenario)

            if task_key in completed_tasks:
                log(f"â­ï¸  [{current_task}/{total_tasks}] åœºæ™¯ {scenario.upper()} å·²å®Œæˆ")
                continue

            progress_pct = (current_task / total_tasks) * 100
            elapsed = time.time() - start_time
            eta = (elapsed / current_task) * (total_tasks - current_task) if current_task > 0 else 0

            log(f"\nğŸ” [{current_task}/{total_tasks}] ({progress_pct:.1f}%) æµ‹è¯•åœºæ™¯: {scenario.upper()}")
            log(f"â±ï¸  å·²ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")

            success, metrics = search_index(M, M_beta, gamma, scenario)

            if success and metrics:
                log(f"   âœ… æµ‹è¯•æˆåŠŸ:")
                log(f"      QPS (with filter): {metrics['qps']:.2f}")
                log(f"      Recall@10: {metrics['recall@10']:.4f}")

                # è®°å½•ç»“æœ
                with open(SUMMARY_FILE, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        M, M_beta, gamma, scenario,
                        f"{build_time:.2f}",
                        f"{metrics['qps']:.2f}",
                        f"{metrics['qps_no_filter']:.2f}",
                        f"{metrics['recall@10']:.4f}",
                        f"{index_size:.1f}",
                        'success',
                        datetime.now().isoformat()
                    ])

                completed_tasks.add(task_key)
                save_progress(completed_tasks)
            else:
                log(f"   âŒ æµ‹è¯•å¤±è´¥")
                # ä¹Ÿè®°å½•å¤±è´¥çš„ç»“æœ
                with open(SUMMARY_FILE, 'a', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        M, M_beta, gamma, scenario,
                        0, 0, 0, 0, 0,
                        'failed',
                        datetime.now().isoformat()
                    ])

    # å®Œæˆæ€»ç»“
    total_time = time.time() - start_time
    log(f"\n{'='*70}")
    log(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    log(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time/3600:.2f}å°æ—¶)")
    log(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {SUMMARY_FILE}")
    log(f"{'='*70}")

    # åˆ†ææœ€ä¼˜å‚æ•°
    analyze_results()

def analyze_results():
    """åˆ†æç»“æœï¼Œæ‰¾å‡ºæœ€ä¼˜å‚æ•°"""
    log(f"\n{'='*70}")
    log("ğŸ“ˆ æœ€ä¼˜å‚æ•°åˆ†æ")
    log(f"{'='*70}\n")

    try:
        import pandas as pd
        df = pd.read_csv(SUMMARY_FILE)
        df = df[df['status'] == 'success']

        for scenario in SCENARIOS:
            log(f"\nåœºæ™¯: {scenario.upper()}")
            log("-" * 50)

            scenario_df = df[df['scenario'] == scenario]

            if len(scenario_df) == 0:
                log("  æ²¡æœ‰æˆåŠŸçš„ç»“æœ")
                continue

            # æŒ‰recall@10æ’åºï¼Œå–å‰5
            top5 = scenario_df.nlargest(5, 'recall@10')

            log(f"  {'M':>3} {'Mb':>3} {'Î³':>2}  {'R@10':>7}  {'QPS':>9}  {'å†…å­˜(MB)':>9}")
            log("  " + "-" * 50)

            for _, row in top5.iterrows():
                log(f"  {int(row['M']):>3} {int(row['M_beta']):>3} {int(row['gamma']):>2}  "
                    f"{row['recall@10']:>7.4f}  {row['qps']:>9.2f}  "
                    f"{row['index_size_mb']:>9.1f}")

            # æ¨èæœ€ä¼˜ï¼ˆrecallæœ€é«˜ä¸”QPSåˆç†ï¼‰
            best = top5.iloc[0]
            log(f"\n  ğŸ† æ¨è: M={int(best['M'])}, M_beta={int(best['M_beta'])}, gamma={int(best['gamma'])}")
            log(f"     Recall@10={best['recall@10']:.4f}, QPS={best['qps']:.2f}")

    except ImportError:
        log("æç¤ºï¼šå®‰è£…pandaså¯ä»¥è‡ªåŠ¨åˆ†ææœ€ä¼˜å‚æ•°")
        log("  pip install pandas")
    except Exception as e:
        log(f"åˆ†æå¤±è´¥: {e}")

if __name__ == '__main__':
    main()
